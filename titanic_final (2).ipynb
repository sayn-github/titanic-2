{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install numpy pandas scikit-learn matplotlib seaborn"
      ],
      "metadata": {
        "id": "ElyTAwY36N6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9bZb6Juwj5Y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('titanic_dataset.csv')"
      ],
      "metadata": {
        "id": "9T6a1I8a6bhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "jjRg2K4T6jow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "5S3qysHi6m5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "TNT9xhBq6m1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "HyWBPhag6mrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(df.isnull(),cmap = 'magma',cbar = False);"
      ],
      "metadata": {
        "id": "RWhWpP8n6v-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "FF7Fb218642Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "duplicate_rows = df[df.duplicated()]\n",
        "duplicate_rows"
      ],
      "metadata": {
        "id": "oomrvot8655P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(['Cabin', 'PassengerId', 'Name', 'Ticket'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "-lRtyCz269x9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EC9iFhTgfE0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### convert the gender to binary 0 and 1\n",
        "df['Sex']=df['Sex'].replace({'male':1,'female':0})\n",
        "df.head()"
      ],
      "metadata": {
        "id": "bry97XX569v_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Embarked']=df['Embarked'].replace({'S':1,'C':2,'Q':3})\n",
        "df.head()"
      ],
      "metadata": {
        "id": "rHG2vCbU69oI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing values in age column by imputing the median\n",
        "df['Age'].fillna(df['Age'].median(), inplace=True)\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "83g4lK3S69r3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing values in embarked column by imputing the mode\n",
        "df[\"Embarked\"].fillna(df[\"Embarked\"].mode()[0], inplace=True)\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "MDAbht67fPLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Family size\n",
        "df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
        "\n",
        "# Is alone\n",
        "df['IsAlone'] = 1  # default to alone\n",
        "df.loc[df['FamilySize'] > 1, 'IsAlone'] = 0\n",
        "\n",
        "# Age bins\n",
        "df['AgeBin'] = pd.cut(df['Age'], bins=[0, 12, 20, 40, 60, 80], labels=False)\n",
        "\n",
        "# Fare bins\n",
        "df['FareBin'] = pd.qcut(df['Fare'], 4, labels=False)\n"
      ],
      "metadata": {
        "id": "XzCB4hlV69kF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(['Age', 'Fare'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "L3SplKcA7z-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qHLa4LUp71aY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Do a Co-Relation analysis among the independent variables and drop variables if they are closely related. (Drop one column and retain another if the co-relation co-efficient is > +-.7)\n",
        "\n",
        "# Calculate the correlation matrix\n",
        "corr_matrix = df.corr()\n",
        "\n",
        "# Create a boolean mask for the upper triangle of the correlation matrix\n",
        "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "\n",
        "# Find columns to drop based on the correlation threshold\n",
        "to_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.7)]\n",
        "\n",
        "# Drop the identified columns\n",
        "df_reduced = df.drop(to_drop, axis=1)\n",
        "\n",
        "print(\"Original columns:\", df.columns.tolist())\n",
        "print(\"Columns to drop due to high correlation:\", to_drop)\n",
        "print(\"Remaining columns after dropping highly correlated ones:\", df_reduced.columns.tolist())\n",
        "\n",
        "# Display the correlation heatmap of the reduced dataframe\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df_reduced.corr(), annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Matrix after dropping highly correlated variables')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "J0A-o8dmnsex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Do standard / min-max scaling on numerical features. (Scale your variables so that no-single variables have more effect on the result)\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "# Identify numerical features for scaling (excluding the target variable 'Survived' if it exists)\n",
        "numerical_features = df_reduced.select_dtypes(include=np.number).columns.tolist()\n",
        "if 'Survived' in numerical_features:\n",
        "    numerical_features.remove('Survived') # Assuming 'Survived' is the target\n",
        "\n",
        "# Standard Scaling (Z-score normalization)\n",
        "scaler_standard = StandardScaler()\n",
        "df_standard_scaled = df_reduced.copy()\n",
        "df_standard_scaled[numerical_features] = scaler_standard.fit_transform(df_standard_scaled[numerical_features])\n",
        "\n",
        "print(\"DataFrame after Standard Scaling:\")\n",
        "print(df_standard_scaled.head())\n",
        "\n",
        "# Min-Max Scaling\n",
        "scaler_minmax = MinMaxScaler()\n",
        "df_minmax_scaled = df_reduced.copy()\n",
        "df_minmax_scaled[numerical_features] = scaler_minmax.fit_transform(df_minmax_scaled[numerical_features])\n",
        "\n",
        "print(\"\\nDataFrame after Min-Max Scaling:\")\n",
        "print(df_minmax_scaled.head())\n",
        "\n",
        "# You can now use either df_standard_scaled or df_minmax_scaled for your downstream modeling."
      ],
      "metadata": {
        "id": "lnyyWavloYAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: divide it into train test split validate\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming 'Survived' is your target variable\n",
        "if 'Survived' in df_reduced.columns:\n",
        "    X = df_reduced.drop('Survived', axis=1)\n",
        "    y = df_reduced['Survived']\n",
        "else:\n",
        "    # Handle the case where 'Survived' is not in the reduced DataFrame\n",
        "    # This might happen if 'Survived' was highly correlated with another feature and dropped\n",
        "    # Or if 'Survived' was not intended to be the target in the reduced set\n",
        "    # For now, let's assume you want to split the entire df_reduced if no target is specified\n",
        "    X = df_reduced\n",
        "    y = None\n",
        "    print(\"Warning: 'Survived' column not found in the reduced dataframe. Splitting the entire dataframe.\")\n",
        "\n",
        "\n",
        "# Split data into training and test sets (e.g., 70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y if y is not None else None) # Use stratify if y is present and for classification tasks\n",
        "\n",
        "# Split the training set further into training and validation sets (e.g., 80% train, 20% validation)\n",
        "# This results in approximately 70% * 80% = 56% train, 70% * 20% = 14% validation, 30% test\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train if y_train is not None else None)\n",
        "\n",
        "print(\"Shapes of the splits:\")\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_val shape:\", X_val.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "if y is not None:\n",
        "    print(\"y_train shape:\", y_train.shape)\n",
        "    print(\"y_val shape:\", y_val.shape)\n",
        "    print(\"y_test shape:\", y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AktbpXKmorRx",
        "outputId": "a17c9347-4805-41d7-b383-0ca75771e2f0"
      },
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes of the splits:\n",
            "X_train shape: (498, 8)\n",
            "X_val shape: (125, 8)\n",
            "X_test shape: (268, 8)\n",
            "y_train shape: (498,)\n",
            "y_val shape: (125,)\n",
            "y_test shape: (268,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df.drop('Survived', axis=1)\n",
        "y = df['Survived']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "QOj6b_gE71W8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: perform logistic regression\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Initialize and train the Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "\n",
        "print('\\nClassification Report:')\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print('\\nConfusion Matrix:')\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "id": "NrQmVJoE71S6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: perform Naive Bayes\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Initialize and train the Gaussian Naive Bayes model\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_nb = nb_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
        "print(f'\\nNaive Bayes Accuracy: {accuracy_nb:.2f}')\n",
        "\n",
        "print('\\nNaive Bayes Classification Report:')\n",
        "print(classification_report(y_test, y_pred_nb))\n",
        "\n",
        "print('\\nNaive Bayes Confusion Matrix:')\n",
        "print(confusion_matrix(y_test, y_pred_nb))\n"
      ],
      "metadata": {
        "id": "vEUXydkG83W3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: perform decision trees\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Initialize and train the Decision Tree model\n",
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "dt_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_dt = dt_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
        "print(f'\\nDecision Tree Accuracy: {accuracy_dt:.2f}')\n",
        "\n",
        "print('\\nDecision Tree Classification Report:')\n",
        "print(classification_report(y_test, y_pred_dt))\n",
        "\n",
        "print('\\nDecision Tree Confusion Matrix:')\n",
        "print(confusion_matrix(y_test, y_pred_dt))"
      ],
      "metadata": {
        "id": "dJUNbsut-EeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: perform random forest\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Initialize and train the Random Forest model\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42) # n_estimators is the number of trees in the forest\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "print(f'\\nRandom Forest Accuracy: {accuracy_rf:.2f}')\n",
        "\n",
        "print('\\nRandom Forest Classification Report:')\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "\n",
        "print('\\nRandom Forest Confusion Matrix:')\n",
        "print(confusion_matrix(y_test, y_pred_rf))"
      ],
      "metadata": {
        "id": "xiQ4EeC5-KR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# param_grid = {\n",
        "#     'n_estimators': [100, 200, 300],         # More trees can increase performance\n",
        "#     'max_depth': [8, 12, 16, None],          # Allow deeper trees\n",
        "#     'min_samples_split': [2, 3, 4],          # Don't over-regularize\n",
        "#     'min_samples_leaf': [1, 2],              # Allow smaller leaves\n",
        "#     'max_features': ['sqrt', 'log2'],        # Control feature selection per split\n",
        "#     'bootstrap': [True]                      # Keep it true for now\n",
        "# }\n",
        "\n",
        "# # Set up Grid Search\n",
        "# grid = GridSearchCV(\n",
        "#     estimator=RandomForestClassifier(random_state=42),\n",
        "#     param_grid=param_grid,\n",
        "#     cv=5,\n",
        "#     scoring='accuracy',\n",
        "#     n_jobs=-1,\n",
        "#     verbose=2\n",
        "# )\n",
        "\n",
        "# # Fit to training data\n",
        "# grid.fit(X_train, y_train)\n",
        "\n",
        "# # Output results\n",
        "# print(\"Best Parameters:\", grid.best_params_)\n",
        "# print(\"Best Cross-Validation Accuracy:\", grid.best_score_)\n",
        "\n",
        "# # Evaluate on test set\n",
        "# best_model = grid.best_estimator_\n",
        "# test_accuracy = best_model.score(X_test, y_test)\n",
        "# print(\"Test Accuracy with tuned parameters:\", test_accuracy)\n"
      ],
      "metadata": {
        "id": "HKHSa0fjTglg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 150, 200],         # Balanced number of trees\n",
        "    'max_depth': [10, 12, 14, 16],           # Slightly deeper trees (not too deep to avoid overfitting)\n",
        "    'min_samples_split': [2, 4, 6],          # Regularization control\n",
        "    'min_samples_leaf': [1, 2, 3],           # Minimum leaf size\n",
        "    'max_features': ['sqrt', 'log2'],        # Feature subset strategies\n",
        "    'bootstrap': [True]                      # Keep bootstrapping\n",
        "}\n",
        "\n",
        "# Set up Grid Search\n",
        "grid = GridSearchCV(\n",
        "    estimator=RandomForestClassifier(random_state=42),\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# Fit to training data\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Output results\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Cross-Validation Accuracy:\", grid.best_score_)\n",
        "\n",
        "# Evaluate on test set\n",
        "best_model = grid.best_estimator_\n",
        "test_accuracy = best_model.score(X_test, y_test)\n",
        "print(\"Test Accuracy with tuned parameters:\", test_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "nFDqX_DZVrv9",
        "outputId": "e3f2f51a-ba13-4410-b781-0ece569d72c4"
      },
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-218-1178653608>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Fit to training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Output results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1022\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1569\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1570\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1571\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    968\u001b[0m                     )\n\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    971\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    972\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2070\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2072\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1682\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1798\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTASK_PENDING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m                 ):\n\u001b[0;32m-> 1800\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1801\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# # Expanded parameter grid\n",
        "# param_grid = {\n",
        "#     'n_estimators': [100, 200, 300, 500],\n",
        "#     'max_depth': [5, 10, 15, 20, None],\n",
        "#     'min_samples_split': [2, 5, 10],\n",
        "#     'min_samples_leaf': [1, 2, 4],\n",
        "#     'bootstrap': [True, False]\n",
        "# }\n",
        "\n",
        "# # GridSearchCV with 5-fold cross-validation\n",
        "# grid = GridSearchCV(estimator=RandomForestClassifier(random_state=42),\n",
        "#                     param_grid=param_grid,\n",
        "#                     cv=5,\n",
        "#                     scoring='accuracy',\n",
        "#                     n_jobs=-1,  # use all processors\n",
        "#                     verbose=2)\n",
        "\n",
        "# grid.fit(X_train, y_train)\n",
        "\n",
        "# print(\"Best Parameters:\", grid.best_params_)\n",
        "# print(\"Best Cross-Validation Score:\", grid.best_score_)\n",
        "\n",
        "# # Use best model to evaluate on test set\n",
        "# best_model = grid.best_estimator_\n",
        "# test_accuracy = best_model.score(X_test, y_test)\n",
        "# print(\"Test Accuracy with tuned parameters:\", test_accuracy)\n"
      ],
      "metadata": {
        "id": "Jm6lXR0yCL7x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}